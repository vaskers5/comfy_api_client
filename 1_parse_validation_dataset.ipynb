{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import numpy as np  # Предполагаем, что используется NumPy\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ExifTags\n",
    "import os\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_ip_adapter_img(item):\n",
    "\n",
    "    # Example string containing URLs\n",
    "    big_string = str(item)\n",
    "\n",
    "    # Regular expression pattern to match the specific URLs\n",
    "    pattern = r'https://cdn\\.lovescape\\.com/cdn/photos/[a-zA-Z0-9/.-]+'\n",
    "\n",
    "    # Find all matches in the big string\n",
    "    matches = re.findall(pattern, big_string)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import time\n",
    "\n",
    "def is_url_expired(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    \n",
    "    if 'Expires' in query_params:\n",
    "        expires_timestamp = int(query_params['Expires'][0])\n",
    "        current_timestamp = int(time.time())\n",
    "        return current_timestamp < expires_timestamp\n",
    "    else:\n",
    "        raise ValueError(\"The URL does not contain an 'Expires' parameter.\")\n",
    "\n",
    "def find_key_in_nested_dict(d, target_key, simple_mode=True, return_all=False):\n",
    "    result_data = []\n",
    "    for d_item in d:\n",
    "        data = _find_key_in_nested_dict(d_item, target_key, simple_mode=simple_mode)\n",
    "        if data:\n",
    "            result_data.extend(data)\n",
    "    if len(result_data) >= 1 and not return_all:\n",
    "        return result_data[0]\n",
    "    elif len(result_data) >= 1 and return_all:\n",
    "        return result_data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_ip_adapter_img(item):\n",
    "\n",
    "    # Example string containing URLs\n",
    "    big_string = str(item)\n",
    "\n",
    "    # Regular expression pattern to match the specific URLs\n",
    "    pattern = r'https://cdn\\.lovescape\\.com/cdn/photos/[a-zA-Z0-9/.-]+'\n",
    "\n",
    "    # Find all matches in the big string\n",
    "    matches = re.findall(pattern, big_string)\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def contains_pattern(string):\n",
    "    # Regular expression to match substrings like __some-text__\n",
    "    pattern = r'__[^_]+__'\n",
    "    # Use re.search to check if the pattern exists in the string\n",
    "    return not(bool(re.search(pattern, string)))\n",
    "\n",
    "\n",
    "def recursive_merge(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Recursively merges dict2 into dict1.\n",
    "    - If a key exists in both dictionaries and the values are also dictionaries, merge them recursively.\n",
    "    - If a key exists in both dictionaries but the values are not dictionaries, overwrite the value in dict1 with the value from dict2.\n",
    "    - If a key exists only in dict2, add it to dict1.\n",
    "\n",
    "    Args:\n",
    "        dict1 (dict): The base dictionary to merge into.\n",
    "        dict2 (dict): The dictionary to merge from.\n",
    "\n",
    "    Returns:\n",
    "        dict: The merged dictionary (modified dict1).\n",
    "    \"\"\"\n",
    "    for key, value in dict2.items():\n",
    "        if key in dict1:\n",
    "            if isinstance(dict1[key], dict) and isinstance(value, dict):\n",
    "                # Both values are dictionaries, merge them recursively\n",
    "                recursive_merge(dict1[key], value)\n",
    "            elif isinstance(dict1[key], list) and isinstance(value, list):\n",
    "                # Both values are lists, extend the list\n",
    "                dict1[key].extend(value)\n",
    "            else:\n",
    "                # Overwrite the value in dict1 with the value from dict2\n",
    "                dict1[key] = value\n",
    "        else:\n",
    "            # Key does not exist in dict1, add it\n",
    "            dict1[key] = value\n",
    "    return dict1\n",
    "\n",
    "\n",
    "def merge_json_from_numpy_list(numpy_list):\n",
    "    json_objects = [] \n",
    "    broken_strings = 0\n",
    "    fragments = {} \n",
    "\n",
    "    broken_strings = 0\n",
    "    # 1. Parse JSON strings into Python dictionaries\n",
    "    for json_string in tqdm(numpy_list, desc=\"Parsing JSON\"):\n",
    "        try:\n",
    "            json_object = json.loads(json_string)\n",
    "            json_objects.append(json_object)\n",
    "            correlation_id = _find_key_in_nested_dict(json_object, 'correlation_id')\n",
    "            correlation_id = [item['str'] for item in correlation_id if 'str' if item]\n",
    "            correlation_id = correlation_id[0]\n",
    "            if correlation_id not in fragments:\n",
    "                fragments[correlation_id] = []\n",
    "            fragments[correlation_id].append(json_object)\n",
    "        except:\n",
    "            broken_strings += 1\n",
    "\n",
    "    return fragments\n",
    "\n",
    "\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def find_key_in_nested_dict(d, target_key, simple_mode=True, return_all=False):\n",
    "    \"\"\"\n",
    "    Wrapper function to find a key in a nested dictionary or list of dictionaries.\n",
    "    \"\"\"\n",
    "    if isinstance(d, list):\n",
    "        # Use generator to avoid creating intermediate lists when not needed\n",
    "        results = chain.from_iterable(\n",
    "            _find_key_in_nested_dict(d_item, target_key, simple_mode=simple_mode) for d_item in d\n",
    "        )\n",
    "    else:\n",
    "        results = _find_key_in_nested_dict(d, target_key, simple_mode=simple_mode)\n",
    "\n",
    "    if return_all:\n",
    "        return list(results)  # Collect all results\n",
    "    return next(results, None)  # Return the first result or None if no match\n",
    "\n",
    "\n",
    "def _find_key_in_nested_dict(d, target_key, simple_mode=True):\n",
    "    \"\"\"\n",
    "    Recursively searches for a key in a nested dictionary or list, yielding results lazily.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a dictionary\n",
    "    if not isinstance(d, dict):\n",
    "        return\n",
    "\n",
    "    for key, value in d.items():\n",
    "        if key == target_key:\n",
    "            if simple_mode and isinstance(value, dict) and len(value) == 1:\n",
    "                yield value\n",
    "            elif not simple_mode:\n",
    "                yield value\n",
    "\n",
    "        # If value is a string, try parsing it as JSON\n",
    "        if isinstance(value, str):\n",
    "            try:\n",
    "                value = json.loads(value)\n",
    "            except json.JSONDecodeError:\n",
    "                pass  # Ignore if not a JSON string\n",
    "\n",
    "        # Recurse into nested dictionaries or lists\n",
    "        if isinstance(value, dict):\n",
    "            yield from _find_key_in_nested_dict(value, target_key, simple_mode=simple_mode)\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    yield from _find_key_in_nested_dict(item, target_key, simple_mode=simple_mode)\n",
    "                elif isinstance(item, str):  # Try parsing strings in lists as JSON\n",
    "                    try:\n",
    "                        item = json.loads(item)\n",
    "                        if isinstance(item, dict):\n",
    "                            yield from _find_key_in_nested_dict(item, target_key, simple_mode=simple_mode)\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab1fde801544180b6af0ef88eed3ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing JSON:   0%|          | 0/1233727 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataframes/new_data_04_12_2025.csv\")\n",
    "# Пример использования\n",
    "numpy_list = np.array(df.message)\n",
    "restored_objects = merge_json_from_numpy_list(numpy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9858bc1bcdb346aeb9d14b7b53364f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58441 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def unwrap_values(data):\n",
    "    # Если это словарь\n",
    "    if isinstance(data, dict):\n",
    "        result = {}\n",
    "        for key, value in data.items():\n",
    "            # Проверяем, если это словарь с одним ключом, который указывает на тип\n",
    "            if isinstance(value, dict) and len(value) == 1 and next(iter(value)) in {'int', 'str', 'float', 'bool', 'array', 'object'}:\n",
    "                result[key] = unwrap_values(next(iter(value.values())))  # Берем значение этого ключа и обрабатываем его\n",
    "            # Если значение - это словарь или список, обрабатываем рекурсивно\n",
    "            else:\n",
    "                result[key] = unwrap_values(value)\n",
    "        return result\n",
    "\n",
    "    # Если это список, обрабатываем каждый элемент\n",
    "    elif isinstance(data, list):\n",
    "        return [unwrap_values(item) for item in data]\n",
    "\n",
    "    # Если это не словарь и не список, возвращаем как есть\n",
    "    return data\n",
    "\n",
    "\n",
    "# Обертка, чтобы избавиться от верхнего уровня 'object'\n",
    "def unwrap_top_level(data):\n",
    "    # Проверяем, если верхний уровень - словарь с ключом 'object'\n",
    "    if isinstance(data, dict) and 'object' in data and isinstance(data['object'], dict):\n",
    "        return unwrap_values(data['object'])  # Возвращаем только содержимое 'object'\n",
    "    return unwrap_values(data)  # Если нет 'object', просто разворачиваем\n",
    "\n",
    "\n",
    "def is_webui_config(item):\n",
    "    if len(item) == 1:\n",
    "        keys = list(item.keys())\n",
    "        return is_webui_config(item[keys[0]])\n",
    "    elif all(value in item for value in ['prompt', 'negative_prompt', 'seed', 'override_settings']):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def extract_webui_config(item):\n",
    "    possible_webui_configs = [*find_key_in_nested_dict(item,'config', return_all=True), *find_key_in_nested_dict(item, 'payload', return_all=True)]\n",
    "    possible_webui_configs = [unwrap_top_level(item) for item in possible_webui_configs]\n",
    "    possible_webui_configs = list(filter(is_webui_config, possible_webui_configs))\n",
    "    if possible_webui_configs:\n",
    "        return possible_webui_configs[0]\n",
    "\n",
    "\n",
    "def extract_usefull_data(item):\n",
    "    \"\"\"\n",
    "    Extracts useful data from a nested dictionary.\n",
    "    \"\"\"\n",
    "    def safe_get(key, default=None, return_all=False, simple_mode=True):\n",
    "        result = find_key_in_nested_dict(item, key, simple_mode=simple_mode, return_all=return_all)\n",
    "        return result if result is not None else default\n",
    "\n",
    "    # result_image_url = safe_get('result_image_urls', {}).get('array', [None])[0]\n",
    "    is_sfw = safe_get('sfw', {}).get('bool', None)\n",
    "    composition = safe_get('composition', {}).get('str', None)\n",
    "    style = safe_get('style', {'str': 'simple'}).get('str', 'simple')\n",
    "    project = safe_get('project', {}).get('str', None)\n",
    "    gender = safe_get('gender', {}).get('str', None)\n",
    "\n",
    "    ip_adapter_img = extract_ip_adapter_img(item)  # Assuming this function is implemented elsewhere\n",
    "    time_stamp = safe_get('datetime', {}).get('str', None)\n",
    "    return {\n",
    "        # \"generation_time\": generation_time,\n",
    "        # \"result_image\": result_image_url,\n",
    "        'webui_config': extract_webui_config(item),\n",
    "        \"ip_adapter_image\": ip_adapter_img,\n",
    "        \"is_sfw\": is_sfw,\n",
    "        \"time_stamp\": time_stamp,\n",
    "        \"composition\": composition,\n",
    "        \"style\": style,\n",
    "        \"project\": project,\n",
    "        \"gender\": gender,\n",
    "    }  \n",
    "\n",
    "\n",
    "# def extract_usefull_data(item):\n",
    "#     \"\"\"\n",
    "#     Extracts useful data from a nested dictionary.\n",
    "#     \"\"\"\n",
    "#     def safe_get(key, default=None, return_all=False, simple_mode=True):\n",
    "#         result = find_key_in_nested_dict(item, key, simple_mode=simple_mode, return_all=return_all)\n",
    "#         return result if result is not None else default\n",
    "\n",
    "#     result_image_url = safe_get('result_image_urls', {}).get('array', [None])[0]\n",
    "#     is_sfw = safe_get('sfw', {}).get('bool', None)\n",
    "#     composition = safe_get('composition', {}).get('str', None)\n",
    "#     # checkpoint_name = safe_get('sd_model_checkpoint', {}).get('str', None)\n",
    "#     style = safe_get('style', {'str': 'simple'}).get('str', 'simple')\n",
    "#     project = safe_get('project', {}).get('str', None)\n",
    "#     gender = safe_get('gender', {}).get('str', None)\n",
    "#     # negative_prompt = safe_get('negative_prompt', {}).get('str', None)\n",
    "#     # positive_prompts_all = find_key_in_nested_dict(item, 'prompt', return_all=True)\n",
    "#     # positive_prompts_all = [item['str'] for item in positive_prompts_all if 'str' in item]\n",
    "#     # positive_prompt = list(filter(contains_pattern, positive_prompts_all))\n",
    "#     # if len(positive_prompt) == 0:\n",
    "#     #     positive_prompt = min(positive_prompts_all, key=len)\n",
    "#     # else:\n",
    "#     #     positive_prompt = positive_prompts_all[0]\n",
    "\n",
    "#     ip_adapter_img = extract_ip_adapter_img(item)  # Assuming this function is implemented elsewhere\n",
    "#     time_stamp = safe_get('datetime', {}).get('str', None)\n",
    "#     generation_time = safe_get('elapsed', {}).get('float', None)\n",
    "#     width = safe_get('width', {'int': 768}).get('int', 768)\n",
    "#     height = safe_get('height', {'int': 1152}).get('int', 1152)\n",
    "\n",
    "#     return {\n",
    "#         \"generation_time\": generation_time,\n",
    "#         \"result_image\": result_image_url,\n",
    "#         \"ip_adapter_image\": ip_adapter_img,\n",
    "#         \"is_sfw\": is_sfw,\n",
    "#         \"time_stamp\": time_stamp,\n",
    "#         \"composition\": composition,\n",
    "#         # \"checkpoint_name\": checkpoint_name,\n",
    "#         \"style\": style,\n",
    "#         \"project\": project,\n",
    "#         \"gender\": gender,\n",
    "#         # \"positive_prompt\": positive_prompt,\n",
    "#         # \"negative_prompt\": negative_prompt,\n",
    "#         \"width\": width,\n",
    "#         \"height\": height,\n",
    "#     }  \n",
    "\n",
    "usefull_data = []\n",
    "lost_data_count = 0\n",
    "lost_data = []\n",
    "\n",
    "for key in tqdm(restored_objects):\n",
    "    # usefull_data.append(extract_usefull_data(item))\n",
    "    try:\n",
    "        item_data = restored_objects[key]\n",
    "        extracted_data = extract_usefull_data(item_data)\n",
    "        extracted_data[\"correlation_id\"] = key\n",
    "        checkp_name = extracted_data['webui_config']['override_settings']['sd_model_checkpoint']\n",
    "        extracted_data[\"generation_type\"] = f\"{'sfw' if extracted_data['is_sfw'] else 'nsfw'}_{extracted_data['composition']}_{extracted_data['gender']}_{extracted_data['style']}_{checkp_name}\"\n",
    "        usefull_data.append(extracted_data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        lost_data_count += 1\n",
    "        lost_data.append(restored_objects[key])\n",
    "        continue\n",
    "pd.DataFrame(usefull_data).to_csv('temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = list(restored_objects.keys())[1269]\n",
    "# extract_usefull_data(restored_objects[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value = restored_objects[list(restored_objects.keys())[0]]\n",
    "# find_key_in_nested_dict(value, 'height')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull_df = pd.DataFrame(usefull_data)\n",
    "usefull_df = pd.read_csv('temp.csv')\n",
    "new_df = usefull_df.loc[usefull_df.result_image.apply(is_url_expired)]\n",
    "# usefull_df['time_stamp'] = pd.to_datetime(usefull_df['time_stamp'])\n",
    "# usefull_df = usefull_df.dropna(subset='ip_adapter_image')\n",
    "# # Сортировка DataFrame по столбцу time_stamp в порядке убывания\n",
    "# usefull_df = usefull_df.sort_values(by='time_stamp', ascending=False)\n",
    "\n",
    "# usefull_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "checkpoint_name\n",
       "clampdxl_v30                    25236\n",
       "dreamshaperXL_v21TurboDPMSDE    21266\n",
       "natvisNaturalVision_v10          8439\n",
       "realisticVisionV51_v51VAE        3420\n",
       "realDream_sdxlPony11               80\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usefull_df.checkpoint_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data for new realistic checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_cjk_symbols(text):\n",
    "#   \"\"\"Removes CJK Unified Ideographs and similar symbols from a string.\n",
    "\n",
    "#   Args:\n",
    "#     text: The input string.\n",
    "\n",
    "#   Returns:\n",
    "#     The string with the specified symbols removed.\n",
    "#   \"\"\"\n",
    "\n",
    "#   # Define the Unicode ranges to exclude\n",
    "#   cjk_ranges = [\n",
    "#       (0x3400, 0x4DBF),  # CJK Unified Ideographs Extension A\n",
    "#       (0x4E00, 0x9FFF),  # CJK Unified Ideographs\n",
    "#       (0x20000, 0x2A6DF),  # CJK Unified Ideographs Extension B\n",
    "#       (0x2A700, 0x2B73F),  # CJK Unified Ideographs Extension C\n",
    "#       (0x2B740, 0x2B81F),  # CJK Unified Ideographs Extension D\n",
    "#       (0x2B820, 0x2CEAF),  # CJK Unified Ideographs Extension E\n",
    "#       (0x2CEB0, 0x2EBEF),  # CJK Unified Ideographs Extension F\n",
    "#       (0xF900, 0xFAFF),  # CJK Compatibility Ideographs\n",
    "#       (0x2F800, 0x2FA1F)  # CJK Compatibility Ideographs Supplement\n",
    "#   ]\n",
    "\n",
    "#   # Filter the string, keeping only characters outside the specified ranges\n",
    "#   filtered_text = \"\".join(char for char in text if not any(\n",
    "#       start <= ord(char) <= end for start, end in cjk_ranges))\n",
    "\n",
    "#   return filtered_text\n",
    "\n",
    "\n",
    "# def extract_meta(img):\n",
    "#     exif = { ExifTags.TAGS[k]: v for k, v in img._getexif().items() if k in ExifTags.TAGS }\n",
    "#     exif[\"UserComment\"] = exif[\"UserComment\"].decode(\"utf-16-be\")\n",
    "#     return exif[\"UserComment\"]\n",
    "\n",
    "\n",
    "# # def parse_image_generation_string(input_string):\n",
    "# #     \"\"\"\n",
    "# #     Parses a string containing image generation parameters and prompts,\n",
    "# #     converting it into a dictionary.\n",
    "\n",
    "# #     Args:\n",
    "# #         input_string: The string to parse.\n",
    "\n",
    "# #     Returns:\n",
    "# #         A dictionary containing the parsed data.\n",
    "# #     \"\"\"\n",
    "\n",
    "# #     data = {}\n",
    "\n",
    "# #     # Split the string into positive prompt, negative prompt, and parameters\n",
    "# #     parts = input_string.split(\"\\nNegative prompt: \")\n",
    "# #     if len(parts) != 2:\n",
    "# #         raise ValueError(\"Invalid string format: Missing 'Negative prompt:' separator.\")\n",
    "\n",
    "# #     positive_prompt = parts[0].strip()\n",
    "# #     data[\"positive_prompt\"] = positive_prompt\n",
    "\n",
    "# #     rest = parts[1].split(\"\\nSteps: \")  # Split at \"Steps:\" to separate prompts from parameters\n",
    "# #     if len(rest) != 2:\n",
    "# #         raise ValueError(\"Invalid string format: Missing 'Steps:' separator.\")\n",
    "\n",
    "# #     negative_prompt = rest[0].strip()\n",
    "# #     data[\"negative_prompt\"] = negative_prompt\n",
    "\n",
    "# #     parameters_str = \"Steps: \" + rest[1]  # Re-add \"Steps:\" to the parameters string\n",
    "# #     parameters = parameters_str.split(\", \")\n",
    "\n",
    "# #     # Parse parameters\n",
    "# #     for param in parameters:\n",
    "# #         if \":\" in param:\n",
    "# #             key, value = param.split(\":\", 1)\n",
    "# #             key = key.strip()\n",
    "# #             value = value.strip()\n",
    "\n",
    "# #             # Handle special cases like ControlNet, Lora hashes, and the remaining parameters\n",
    "# #             if key.startswith(\"ControlNet\"):\n",
    "# #                 # Extract ControlNet details into a sub-dictionary\n",
    "# #                 controlnet_data = {}\n",
    "# #                 controlnet_parts = value.split(\", \")\n",
    "# #                 for part in controlnet_parts:\n",
    "# #                     if \":\" in part:\n",
    "# #                         cn_key, cn_value = part.split(\":\", 1)\n",
    "# #                         controlnet_data[cn_key.strip()] = cn_value.strip().strip('\"')  # Remove quotes\n",
    "# #                 data[key] = controlnet_data\n",
    "\n",
    "# #             elif key == \"Lora hashes\":\n",
    "# #                 # Extract Lora hashes into a dictionary\n",
    "# #                 lora_data = {}\n",
    "# #                 lora_parts = value.split(\", \")\n",
    "# #                 for part in lora_parts:\n",
    "# #                     if \":\" in part:\n",
    "# #                         lora_name, lora_hash = part.split(\":\", 1)\n",
    "# #                         lora_data[lora_name.strip()] = lora_hash.strip()\n",
    "# #                 data[key] = lora_data\n",
    "\n",
    "# #             else:\n",
    "# #                 data[key] = value\n",
    "\n",
    "# #     return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_img_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img\n",
    "\n",
    "def load_photos(sdwebui_request, load_dir: str):\n",
    "\n",
    "    os.makedirs(load_dir, exist_ok=True)\n",
    "    webui_img_path = f\"{load_dir}/sdwebui_img.png\"\n",
    "    ip_adapter_img = None\n",
    "    ip_adapter_img_path = None\n",
    "    if not(pd.isna(sdwebui_request['ip_adapter_image'])):\n",
    "        ip_adapter_img_path = f\"{load_dir}/ip_adapter_img.png\"\n",
    "        ip_adapter_img = load_single_img_from_url(sdwebui_request[\"ip_adapter_image\"])\n",
    "        ip_adapter_img.save(ip_adapter_img_path)\n",
    "    return {\n",
    "        \"webui_img_path\": os.path.abspath(webui_img_path),\n",
    "        \"ip_adapter_img_path\": os.path.abspath(ip_adapter_img_path) if ip_adapter_img_path else None,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "\n",
    "ban_checkpoints = ['realisticVisionV51_v51VAE']\n",
    "\n",
    "def save_and_load_data(sample_name: str, ban_checkpoints: list[str], sampling_df: pd.DataFrame, k: int):\n",
    "    sampling_df = sampling_df.loc[~sampling_df.checkpoint_name.isin(ban_checkpoints)]\n",
    "    used_checkpoints = sampling_df.generation_type.value_counts().keys().tolist()\n",
    "    dfs = [sample_generation_type(generation_type=generation_type, df=sampling_df, k=k) for generation_type in used_checkpoints]\n",
    "    sampled_df = pd.concat(dfs)\n",
    "    print(sampled_df.generation_type.value_counts())\n",
    "    print(len(sampled_df[sampled_df.ip_adapter_image.isna()]), len(sampled_df[~sampled_df.ip_adapter_image.isna()]))\n",
    "    sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    cur_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    exp_data_folder = f\"photos_loaded/{sample_name}_load_data_{cur_date}\"\n",
    "    shutil.rmtree(exp_data_folder, ignore_errors=True)\n",
    "\n",
    "    result_data = []\n",
    "    for i in tqdm(range(len(sampled_df))):\n",
    "        sdwebui_test = sampled_df.iloc[i]\n",
    "        resp = load_photos(sdwebui_test, f\"{exp_data_folder}/item_{i}\")\n",
    "        result_data.append(resp)\n",
    "\n",
    "\n",
    "    sampled_df = pd.concat([sampled_df, pd.DataFrame(result_data)], axis=1)\n",
    "    csv_path = f'dataframes/{sample_name}_{cur_date}.csv'\n",
    "    sampled_df.to_csv(csv_path, index=False)\n",
    "    print(csv_path)\n",
    "\n",
    "\n",
    "def check_ip_adapter_image(item):\n",
    "    return isinstance(item, str) and \"cdn\" in item\n",
    "\n",
    "def sample_generation_type(generation_type, df, k=25):\n",
    "    def sampled_if(item, k):\n",
    "        return item.sample(n=k if len(item) > k else len(item), random_state=42)\n",
    "    \n",
    "\n",
    "    cp_df = df[df.generation_type == generation_type]\n",
    "    ip_image_df = cp_df.loc[cp_df.ip_adapter_image.apply(check_ip_adapter_image)]\n",
    "    without_ip_image_df = cp_df.loc[~cp_df.ip_adapter_image.apply(check_ip_adapter_image)]\n",
    "    if len(ip_image_df) > 0:\n",
    "        sampled_data = pd.concat([sampled_if(ip_image_df, k), sampled_if(without_ip_image_df, k)])\n",
    "        return sampled_data\n",
    "    else:\n",
    "        return sampled_if(cp_df, 2*k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df['ip_adapter_image'] = new_df.ip_adapter_image.apply(lambda item: item[0] if item is None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import os\n",
    "# # import pandas as pd\n",
    "# # import requests\n",
    "# # from PIL import Image\n",
    "# # from io import BytesIO\n",
    "# # from datetime import datetime\n",
    "# # import shutil\n",
    "# # from tqdm import tqdm\n",
    "# # from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "# def sample_generation_type(generation_type, df, k=25):\n",
    "#     def sampled_if(item, k):\n",
    "#         return item.sample(n=k if len(item) > k else len(item), random_state=42)\n",
    "    \n",
    "\n",
    "#     cp_df = df[df.generation_type == generation_type]\n",
    "#     ip_image_df = cp_df.loc[cp_df.ip_adapter_image.apply(check_ip_adapter_image)]\n",
    "#     without_ip_image_df = cp_df.loc[~cp_df.ip_adapter_image.apply(check_ip_adapter_image)]\n",
    "#     if len(ip_image_df) > 0:\n",
    "#         sampled_data = pd.concat([sampled_if(ip_image_df, k), sampled_if(without_ip_image_df, k)])\n",
    "#         return sampled_data\n",
    "#     else:\n",
    "#         return sampled_if(cp_df, 2*k)\n",
    "\n",
    "# # # Function to check IP adapter image validity\n",
    "# # def check_ip_adapter_image(item):\n",
    "# #     return isinstance(item, str) and \"cdn\" in item\n",
    "\n",
    "\n",
    "# # # Function to sample generation types\n",
    "# # def sample_generation_type(generation_type, df, k=25):\n",
    "# #     def sampled_if(item, k):\n",
    "# #         return item.sample(n=k if len(item) > k else len(item), random_state=42)\n",
    "\n",
    "# #     cp_df = df[df.generation_type == generation_type]\n",
    "# #     ip_image_df = cp_df[cp_df.ip_adapter_image.apply(check_ip_adapter_image)]\n",
    "# #     without_ip_image_df = cp_df[~cp_df.ip_adapter_image.apply(check_ip_adapter_image)]\n",
    "    \n",
    "# #     if len(ip_image_df) > 0:\n",
    "# #         sampled_data = pd.concat([sampled_if(ip_image_df, k), sampled_if(without_ip_image_df, k)])\n",
    "# #         return sampled_data\n",
    "# #     else:\n",
    "# #         return sampled_if(cp_df, 2 * k)\n",
    "\n",
    "\n",
    "# # # Optimized function to load a single image from a URL\n",
    "# # def load_single_img_from_url(url):\n",
    "# #     try:\n",
    "# #         response = requests.get(url, stream=True, timeout=10)  # Use streaming and set timeout\n",
    "# #         response.raise_for_status()  # Raise error for HTTP issues\n",
    "# #         return Image.open(BytesIO(response.content))\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"Failed to load image from URL {url}: {e}\")\n",
    "# #         return None\n",
    "\n",
    "# # # Optimized function to load photos\n",
    "# # def load_photos(sdwebui_request, load_dir):\n",
    "# #     \"\"\"Loads images from the provided URLs and saves them to the specified directory.\"\"\"\n",
    "# #     os.makedirs(load_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# #     # Load the main web UI image\n",
    "# #     sdwebui_img = load_single_img_from_url(sdwebui_request.get(\"result_image\"))\n",
    "# #     if sdwebui_img is None:  # Handle failures gracefully\n",
    "# #         return {\n",
    "# #             \"positive_prompt\": None,\n",
    "# #             \"negative_prompt\": None,\n",
    "# #             \"webui_img_path\": None,\n",
    "# #             \"ip_adapter_img_path\": None,\n",
    "# #         }\n",
    "\n",
    "# #     webui_img_path = os.path.join(load_dir, \"sdwebui_img.png\")\n",
    "# #     sdwebui_img.save(webui_img_path)  # Save the main image\n",
    "\n",
    "# #     # Load the IP adapter image if available\n",
    "# #     ip_adapter_img_path = None\n",
    "# #     ip_adapter_image_url = sdwebui_request.get(\"ip_adapter_image\")\n",
    "# #     if ip_adapter_image_url and not pd.isna(ip_adapter_image_url):  # Check for valid URL\n",
    "# #         ip_adapter_img = load_single_img_from_url(ip_adapter_image_url)\n",
    "# #         if ip_adapter_img:\n",
    "# #             ip_adapter_img_path = os.path.join(load_dir, \"ip_adapter_img.png\")\n",
    "# #             ip_adapter_img.save(ip_adapter_img_path)\n",
    "\n",
    "# #     # Return paths to the saved images\n",
    "# #     return {\n",
    "# #         \"webui_img_path\": webui_img_path,\n",
    "# #         \"ip_adapter_img_path\": ip_adapter_img_path,\n",
    "# #     }\n",
    "\n",
    "# # # Optimized function to save and load data\n",
    "# # def save_and_load_data(sample_name, ban_checkpoints, sampling_df, k):\n",
    "# #     \"\"\"Filters, samples, and processes data, saving results to disk.\"\"\"\n",
    "# #     # Filter out banned checkpoints\n",
    "# #     sampling_df = sampling_df[~sampling_df[\"checkpoint_name\"].isin(ban_checkpoints)]\n",
    "\n",
    "# #     # Get unique generation types and sample k items per type\n",
    "# #     used_checkpoints = sampling_df[\"generation_type\"].unique()\n",
    "# #     sampled_dfs = [\n",
    "# #         sample_generation_type(generation_type, sampling_df, k) for generation_type in used_checkpoints\n",
    "# #     ]\n",
    "# #     sampled_df = pd.concat(sampled_dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# #     # Prepare the output folder\n",
    "# #     cur_date = datetime.now().strftime('%Y-%m-%d')\n",
    "# #     exp_data_folder = f\"img_data/load_data_{cur_date}\"\n",
    "# #     shutil.rmtree(exp_data_folder, ignore_errors=True)\n",
    "\n",
    "# #     # Process the data in parallel\n",
    "# #     result_data = []\n",
    "# #     os.makedirs(exp_data_folder, exist_ok=True)\n",
    "# #     with ThreadPoolExecutor() as executor:\n",
    "# #         # Submit tasks for image loading and saving\n",
    "# #         futures = [\n",
    "# #             executor.submit(load_photos, sampled_df.iloc[i], f\"{exp_data_folder}/item_{i}\")\n",
    "# #             for i in range(len(sampled_df))\n",
    "# #         ]\n",
    "# #         for future in tqdm(futures, desc=\"Processing Images\"):\n",
    "# #             result_data.append(future.result())\n",
    "\n",
    "# #     # Combine results and save to CSV\n",
    "# #     result_df = pd.DataFrame(result_data)\n",
    "# #     final_df = pd.concat([sampled_df, result_df], axis=1)\n",
    "# #     output_file = f\"dataframes/{sample_name}_{cur_date}.csv\"\n",
    "# #     final_df.to_csv(output_file, index=False)\n",
    "# #     print(f\"Data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ban_checkpoints = ['realisticVisionV51_v51VAE']\n",
    "# save_and_load_data('inference', ban_checkpoints, new_df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generation_type\n",
      "nsfw_full_None_simple_natvisNaturalVision_v10      200\n",
      "nsfw_custom_None_simple_natvisNaturalVision_v10    200\n",
      "Name: count, dtype: int64\n",
      "200 200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85edf79420e042be86843d81419d828a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframes/natvis_solo_test_2024-12-06.csv\n"
     ]
    }
   ],
   "source": [
    "ban_checkpoints = ['realisticVisionV51_v51VAE', 'clampdxl_v30', 'dreamshaperXL_v21TurboDPMSDE', 'realDream_sdxlPony11']\n",
    "save_and_load_data('natvis_solo_test', ban_checkpoints, new_df, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
